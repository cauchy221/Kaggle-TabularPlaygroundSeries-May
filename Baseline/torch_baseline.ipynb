{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-17T18:02:09.652433Z","iopub.execute_input":"2022-09-17T18:02:09.652971Z","iopub.status.idle":"2022-09-17T18:02:09.664897Z","shell.execute_reply.started":"2022-09-17T18:02:09.652933Z","shell.execute_reply":"2022-09-17T18:02:09.663674Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv\n/kaggle/input/tabular-playground-series-may-2022/train.csv\n/kaggle/input/tabular-playground-series-may-2022/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Pytorch开发深度学习模型一般步骤\n\n## 1.定义DataSet\n- 理解数据原始形式\n- 理解数据编码方式\n- 理解如何进行数据I/O\n\n## 2.定义模型\n- 定义各个子模块\n- 将子模块合并成最终的模型\n\n## 3.完成Train Pipeline/Valid Pipeline\n- Pytorch一般的Train Pipeline/Valid Pipeline书写","metadata":{}},{"cell_type":"code","source":"# 导入库\nimport torch\nfrom torch.utils.data import Dataset\nimport torch.utils.data as D\nfrom torch import nn\nimport copy\nimport os\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict  # 该模块提供高阶数据类型（有默认值的字典）","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:02:09.666822Z","iopub.execute_input":"2022-09-17T18:02:09.667253Z","iopub.status.idle":"2022-09-17T18:02:09.677262Z","shell.execute_reply.started":"2022-09-17T18:02:09.667206Z","shell.execute_reply":"2022-09-17T18:02:09.675197Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# 参数\nconfig = {\n    'train_path': '/kaggle/input/tabular-playground-series-may-2022/train.csv', \n    'test_path': '/kaggle/input/tabular-playground-series-may-2022/test.csv',\n    \"sparse_cols\":['f_07','f_08','f_09','f_10','f_11','f_12','f_13','f_14','f_15','f_16','f_17','f_18','f_29','f_30']+[f'ch_{i}' for i in range(10)],\n    \"dense_cols\": ['f_00','f_01','f_02','f_03','f_04','f_05','f_06','f_19','f_20','f_21','f_22','f_23','f_24','f_25','f_26','f_28',],\n    \"debug_mode\": False,\n    \"epoch\": 5,\n    \"batch\": 2048,\n    \"lr\": 0.001,\n    \"device\": 0,\n}","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:02:09.679359Z","iopub.execute_input":"2022-09-17T18:02:09.680297Z","iopub.status.idle":"2022-09-17T18:02:09.697803Z","shell.execute_reply.started":"2022-09-17T18:02:09.680254Z","shell.execute_reply":"2022-09-17T18:02:09.696218Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(config['train_path'])\nif config['debug_mode']:\n    train_df = train_df[:1000]\ntest_df = pd.read_csv(config['test_path'])\n\ndf = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n\nfor i in tqdm(range(10)):\n    df[f'ch_{i}'] = df['f_27'].str.get(i).apply(ord) - ord('A')","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:02:09.700343Z","iopub.execute_input":"2022-09-17T18:02:09.701817Z","iopub.status.idle":"2022-09-17T18:02:39.234646Z","shell.execute_reply.started":"2022-09-17T18:02:09.701765Z","shell.execute_reply":"2022-09-17T18:02:39.233592Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de3642e70724d53b77748cd25755128"}},"metadata":{}}]},{"cell_type":"code","source":"# 特征编码函数\ndef get_enc_dict(df, config):\n    enc_dict = defaultdict(dict)  # 元素数据类型为dict\n    for f in tqdm(config['sparse_cols']):  # 离散型数据\n        map_dict = dict(zip(df[f].unique(), range(1, df[f].nunique()+1)))\n        enc_dict[f] = map_dict\n        enc_dict[f]['vocab_size'] = df[f].unique()+1\n        \n    for f in tqdm(config['dense_cols']):  # 连续型数据\n        enc_dict[f]['min'] = df[f].min()\n        enc_dict[f]['max'] = df[f].max()\n        enc_dict[f]['std'] = df[f].std()\n        \n    return enc_dict","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:02:39.236375Z","iopub.execute_input":"2022-09-17T18:02:39.237122Z","iopub.status.idle":"2022-09-17T18:02:39.247593Z","shell.execute_reply.started":"2022-09-17T18:02:39.237083Z","shell.execute_reply":"2022-09-17T18:02:39.245936Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"enc_dict = get_enc_dict(df, config)\nprint(enc_dict)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:02:39.249908Z","iopub.execute_input":"2022-09-17T18:02:39.250457Z","iopub.status.idle":"2022-09-17T18:02:40.669552Z","shell.execute_reply.started":"2022-09-17T18:02:39.250402Z","shell.execute_reply":"2022-09-17T18:02:40.668037Z"},"trusted":true},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5766a34108194d76af6dc8ca94c5b926"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40d74832938c48459d7d22bb0a5b7bac"}},"metadata":{}},{"name":"stdout","text":"defaultdict(<class 'dict'>, {'f_07': {1: 1, 3: 2, 6: 3, 2: 4, 5: 5, 4: 6, 0: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 'vocab_size': array([ 2,  4,  7,  3,  6,  5,  1,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])}, 'f_08': {5: 1, 3: 2, 0: 3, 2: 4, 1: 5, 7: 6, 6: 7, 4: 8, 9: 9, 8: 10, 10: 11, 11: 12, 12: 13, 13: 14, 16: 15, 14: 16, 'vocab_size': array([ 6,  4,  1,  3,  2,  8,  7,  5, 10,  9, 11, 12, 13, 14, 17, 15])}, 'f_09': {1: 1, 4: 2, 2: 3, 0: 4, 3: 5, 5: 6, 7: 7, 6: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 16: 16, 'vocab_size': array([ 2,  5,  3,  1,  4,  6,  8,  7,  9, 10, 11, 12, 13, 14, 15, 17])}, 'f_10': {3: 1, 0: 2, 6: 3, 4: 4, 2: 5, 7: 6, 1: 7, 5: 8, 10: 9, 8: 10, 9: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 'vocab_size': array([ 4,  1,  7,  5,  3,  8,  2,  6, 11,  9, 10, 12, 13, 14, 15, 16])}, 'f_11': {3: 1, 2: 2, 6: 3, 1: 4, 4: 5, 5: 6, 0: 7, 9: 8, 7: 9, 8: 10, 10: 11, 12: 12, 11: 13, 13: 14, 14: 15, 'vocab_size': array([ 4,  3,  7,  2,  5,  6,  1, 10,  8,  9, 11, 13, 12, 14, 15])}, 'f_12': {3: 1, 4: 2, 6: 3, 0: 4, 1: 5, 2: 6, 5: 7, 7: 8, 8: 9, 9: 10, 12: 11, 10: 12, 11: 13, 13: 14, 14: 15, 16: 16, 15: 17, 'vocab_size': array([ 4,  5,  7,  1,  2,  3,  6,  8,  9, 10, 13, 11, 12, 14, 15, 17, 16])}, 'f_13': {1: 1, 0: 2, 3: 3, 4: 4, 6: 5, 5: 6, 2: 7, 8: 8, 7: 9, 10: 10, 9: 11, 11: 12, 12: 13, 13: 14, 'vocab_size': array([ 2,  1,  4,  5,  7,  6,  3,  9,  8, 11, 10, 12, 13, 14])}, 'f_14': {6: 1, 1: 2, 2: 3, 0: 4, 3: 5, 5: 6, 4: 7, 7: 8, 8: 9, 9: 10, 11: 11, 10: 12, 14: 13, 12: 14, 'vocab_size': array([ 7,  2,  3,  1,  4,  6,  5,  8,  9, 10, 12, 11, 15, 13])}, 'f_15': {1: 1, 0: 2, 2: 3, 3: 4, 6: 5, 4: 6, 9: 7, 5: 8, 7: 9, 8: 10, 10: 11, 14: 12, 11: 13, 12: 14, 13: 15, 'vocab_size': array([ 2,  1,  3,  4,  7,  5, 10,  6,  8,  9, 11, 15, 12, 13, 14])}, 'f_16': {0: 1, 4: 2, 2: 3, 3: 4, 1: 5, 5: 6, 7: 7, 6: 8, 8: 9, 9: 10, 11: 11, 10: 12, 13: 13, 12: 14, 15: 15, 14: 16, 'vocab_size': array([ 1,  5,  3,  4,  2,  6,  8,  7,  9, 10, 12, 11, 14, 13, 16, 15])}, 'f_17': {7: 1, 6: 2, 1: 3, 0: 4, 4: 5, 3: 6, 2: 7, 5: 8, 9: 9, 8: 10, 11: 11, 12: 12, 10: 13, 14: 14, 13: 15, 'vocab_size': array([ 8,  7,  2,  1,  5,  4,  3,  6, 10,  9, 12, 13, 11, 15, 14])}, 'f_18': {4: 1, 0: 2, 3: 3, 2: 4, 1: 5, 6: 6, 5: 7, 7: 8, 9: 9, 8: 10, 10: 11, 11: 12, 12: 13, 13: 14, 'vocab_size': array([ 5,  1,  4,  3,  2,  7,  6,  8, 10,  9, 11, 12, 13, 14])}, 'f_29': {0: 1, 1: 2, 'vocab_size': array([1, 2])}, 'f_30': {0: 1, 2: 2, 1: 3, 'vocab_size': array([1, 3, 2])}, 'ch_0': {0: 1, 1: 2, 'vocab_size': array([1, 2])}, 'ch_1': {1: 1, 2: 2, 0: 3, 3: 4, 4: 5, 6: 6, 5: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 'vocab_size': array([ 2,  3,  1,  4,  5,  7,  6,  8,  9, 10, 11, 12, 13, 14, 15])}, 'ch_2': {0: 1, 1: 2, 'vocab_size': array([1, 2])}, 'ch_3': {1: 1, 2: 2, 4: 3, 3: 4, 7: 5, 8: 6, 6: 7, 0: 8, 5: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 'vocab_size': array([ 2,  3,  5,  4,  8,  9,  7,  1,  6, 10, 11, 12, 13, 14, 15])}, 'ch_4': {3: 1, 2: 2, 0: 3, 1: 4, 5: 5, 4: 6, 6: 7, 8: 8, 7: 9, 9: 10, 10: 11, 11: 12, 12: 13, 14: 14, 13: 15, 'vocab_size': array([ 4,  3,  1,  2,  6,  5,  7,  9,  8, 10, 11, 12, 13, 15, 14])}, 'ch_5': {0: 1, 1: 2, 'vocab_size': array([1, 2])}, 'ch_6': {3: 1, 2: 2, 0: 3, 4: 4, 1: 5, 6: 6, 7: 7, 5: 8, 8: 9, 9: 10, 12: 11, 10: 12, 11: 13, 13: 14, 14: 15, 'vocab_size': array([ 4,  3,  1,  5,  2,  7,  8,  6,  9, 10, 13, 11, 12, 14, 15])}, 'ch_7': {1: 1, 2: 2, 10: 3, 7: 4, 4: 5, 15: 6, 3: 7, 0: 8, 18: 9, 19: 10, 9: 11, 16: 12, 13: 13, 17: 14, 6: 15, 12: 16, 5: 17, 14: 18, 8: 19, 11: 20, 'vocab_size': array([ 2,  3, 11,  8,  5, 16,  4,  1, 19, 20, 10, 17, 14, 18,  7, 13,  6,\n       15,  9, 12])}, 'ch_8': {0: 1, 4: 2, 2: 3, 5: 4, 3: 5, 7: 6, 1: 7, 6: 8, 8: 9, 9: 10, 11: 11, 10: 12, 12: 13, 13: 14, 14: 15, 'vocab_size': array([ 1,  5,  3,  6,  4,  8,  2,  7,  9, 10, 12, 11, 13, 14, 15])}, 'ch_9': {1: 1, 3: 2, 4: 3, 0: 4, 2: 5, 10: 6, 5: 7, 6: 8, 7: 9, 9: 10, 8: 11, 11: 12, 12: 13, 13: 14, 14: 15, 'vocab_size': array([ 2,  4,  5,  1,  3, 11,  6,  7,  8, 10,  9, 12, 13, 14, 15])}, 'f_00': {'min': -4.6580176066619465, 'max': 5.761949765720581, 'std': 0.9994897558802066}, 'f_01': {'min': -4.9227183871002, 'max': 4.81569922916624, 'std': 0.9982911706633691}, 'f_02': {'min': -4.642676261581941, 'max': 4.961982157037259, 'std': 1.0004279262219873}, 'f_03': {'min': -4.658816423702026, 'max': 4.899903807128348, 'std': 1.0008678959557167}, 'f_04': {'min': -4.7485008094997285, 'max': 4.948982867422749, 'std': 0.9996948834723032}, 'f_05': {'min': -5.1413562085948, 'max': 4.9718811207257465, 'std': 1.000038219485786}, 'f_06': {'min': -4.84291865900587, 'max': 4.82266765638437, 'std': 0.9997653455997301}, 'f_19': {'min': -11.419021060925678, 'max': 12.079667330851391, 'std': 2.3155464557902468}, 'f_20': {'min': -11.257916931897675, 'max': 11.47532524994984, 'std': 2.399376529415103}, 'f_21': {'min': -13.310145923543772, 'max': 14.455426205132513, 'std': 2.484483326645232}, 'f_22': {'min': -11.85352955078141, 'max': 11.344080226183534, 'std': 2.4508454371003885}, 'f_23': {'min': -12.301097278095106, 'max': 12.247100149129777, 'std': 2.45306076419903}, 'f_24': {'min': -11.838417121429853, 'max': 12.389843507506578, 'std': 2.387814285168815}, 'f_25': {'min': -13.312783903022751, 'max': 12.529178899213573, 'std': 2.4172192028695068}, 'f_26': {'min': -14.300576720342912, 'max': 12.913041459864832, 'std': 2.4767603555713453}, 'f_28': {'min': -1229.7530519319007, 'max': 1229.5625773272975, 'std': 238.82420955525262}})\n","output_type":"stream"}]},{"cell_type":"code","source":"# 构造dataset\nclass BaseDataset(Dataset):\n    def __init__(self, df, config, enc_dict=None):\n        self.df = df\n        self.config = config\n        self.enc_dict = enc_dict\n        self.dense_cols = list(set(self.config['dense_cols']))\n        self.sparse_cols = list(set(self.config['sparse_cols']))\n        self.feature_name = self.dense_cols + self.sparse_cols + ['label']\n        self.enc_data()  # 自动执行数据编码函数，得到self.enc_df\n        \n    def enc_dense_data(self, col):\n        return (self.df[col] - self.enc_dict[col]['min']) / (self.enc_dict[col]['max'] - self.enc_dict[col]['min'])\n    \n    def enc_sparse_data(self, col):\n        return self.df[col].apply(lambda x: self.enc_dict[col].get(x,0))\n    \n    def enc_data(self):\n        self.enc_df = copy.deepcopy(self.df)  # 深拷贝\n        for col in self.dense_cols:\n            self.enc_df[col] = self.enc_dense_data(col)\n        for col in self.sparse_cols:\n            self.enc_df[col] = self.enc_sparse_data(col)\n            \n    def __getitem__(self, index):  # 根据index取数据\n        data = dict()\n        for col in self.feature_name:\n            if col in self.dense_cols:\n                data[col] = torch.Tensor([self.enc_df[col].iloc[index]]).squeeze(-1)\n            elif col in self.sparse_cols:\n                data[col] = torch.Tensor([self.enc_df[col].iloc[index]]).long().squeeze(-1)\n            \n        if 'target' in self.enc_df.columns:\n            data['target'] = torch.Tensor([self.enc_df['target'].iloc[index]]).squeeze(-1)\n        return data\n    \n    def __len__(self):\n        return len(self.enc_df)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:02:40.672567Z","iopub.execute_input":"2022-09-17T18:02:40.672985Z","iopub.status.idle":"2022-09-17T18:02:40.690186Z","shell.execute_reply.started":"2022-09-17T18:02:40.672949Z","shell.execute_reply":"2022-09-17T18:02:40.688503Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"train_df = df[df['target'].notna()].reset_index(drop=True)\ntest_df = df[df['target'].isna()].reset_index(drop=True)\n\ntrain_num = int(len(train_df)*0.8)  # 按照8:2划分训练集和验证集\n\nvalid_df = train_df[train_num:].reset_index(drop=True)\ntrain_df = train_df[:train_num].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:02:40.692214Z","iopub.execute_input":"2022-09-17T18:02:40.692666Z","iopub.status.idle":"2022-09-17T18:02:41.806699Z","shell.execute_reply.started":"2022-09-17T18:02:40.692613Z","shell.execute_reply":"2022-09-17T18:02:41.805477Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"train_dataset = BaseDataset(train_df, config, enc_dict=enc_dict)\nvalid_dataset = BaseDataset(valid_df, config, enc_dict=enc_dict)\ntest_dataset = BaseDataset(test_df, config, enc_dict=enc_dict)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:02:41.808488Z","iopub.execute_input":"2022-09-17T18:02:41.809023Z","iopub.status.idle":"2022-09-17T18:03:08.433811Z","shell.execute_reply.started":"2022-09-17T18:02:41.808973Z","shell.execute_reply":"2022-09-17T18:03:08.432588Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# 可以抽取出来看看\ntrain_dataset.__getitem__(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:08.435382Z","iopub.execute_input":"2022-09-17T18:03:08.435796Z","iopub.status.idle":"2022-09-17T18:03:08.457841Z","shell.execute_reply.started":"2022-09-17T18:03:08.435759Z","shell.execute_reply":"2022-09-17T18:03:08.456583Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"{'f_21': tensor(0.4961),\n 'f_22': tensor(0.4136),\n 'f_00': tensor(0.5766),\n 'f_03': tensor(0.4239),\n 'f_01': tensor(0.6185),\n 'f_06': tensor(0.3839),\n 'f_20': tensor(0.6430),\n 'f_23': tensor(0.4478),\n 'f_26': tensor(0.5356),\n 'f_05': tensor(0.5364),\n 'f_02': tensor(0.5420),\n 'f_28': tensor(0.5187),\n 'f_19': tensor(0.5284),\n 'f_25': tensor(0.5545),\n 'f_04': tensor(0.6201),\n 'f_24': tensor(0.3972),\n 'ch_7': tensor(5),\n 'f_15': tensor(1),\n 'ch_8': tensor(5),\n 'f_13': tensor(3),\n 'ch_1': tensor(4),\n 'f_09': tensor(2),\n 'f_07': tensor(3),\n 'f_08': tensor(4),\n 'ch_3': tensor(3),\n 'f_11': tensor(4),\n 'ch_5': tensor(2),\n 'f_18': tensor(4),\n 'ch_9': tensor(4),\n 'ch_2': tensor(1),\n 'f_14': tensor(2),\n 'f_16': tensor(5),\n 'f_29': tensor(1),\n 'f_17': tensor(4),\n 'f_30': tensor(3),\n 'f_12': tensor(3),\n 'f_10': tensor(5),\n 'ch_0': tensor(2),\n 'ch_6': tensor(1),\n 'ch_4': tensor(1),\n 'target': tensor(0.)}"},"metadata":{}}]},{"cell_type":"code","source":"# 定义模型基本层\nclass EmbeddingLayer(nn.Module):\n    def __init__(self,\n                 enc_dict = None,\n                 embedding_dim = None):\n        super(EmbeddingLayer, self).__init__()\n        self.enc_dict = enc_dict\n        self.embedding_dim = embedding_dim\n        self.embedding_layer = nn.ModuleDict()\n\n        self.emb_feature = []\n\n        for col in self.enc_dict.keys():\n            if 'vocab_size' in self.enc_dict[col].keys():\n                self.emb_feature.append(col)\n                self.embedding_layer.update({col : nn.Embedding(\n                    self.enc_dict[col]['vocab_size'],\n                    self.embedding_dim,\n                )})\n\n    def forward(self, X):\n        #对所有的sparse特征挨个进行embedding\n        feature_emb_list = []\n        for col in self.emb_feature:\n            inp = X[col].long().view(-1, 1)\n            feature_emb_list.append(self.embedding_layer[col](inp))\n        feature_emb = torch.stack(feature_emb_list, dim=1)\n        return feature_emb","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:08.459433Z","iopub.execute_input":"2022-09-17T18:03:08.459854Z","iopub.status.idle":"2022-09-17T18:03:08.471640Z","shell.execute_reply.started":"2022-09-17T18:03:08.459819Z","shell.execute_reply":"2022-09-17T18:03:08.470199Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# 定义多层感知机模型\nclass MLP_Layer(nn.Module):\n    def __init__(self,\n                 input_dim,\n                 output_dim=None,\n                 hidden_units=[],\n                 hidden_activations=\"ReLU\",\n                 final_activation=None,\n                 dropout_rates=0,\n                 batch_norm=False,\n                 use_bias=True):\n        super(MLP_Layer, self).__init__()\n        dense_layers = []\n        if not isinstance(dropout_rates, list):\n            dropout_rates = [dropout_rates] * len(hidden_units)\n        if not isinstance(hidden_activations, list):\n            hidden_activations = [hidden_activations] * len(hidden_units)\n        hidden_activations = [set_activation(x) for x in hidden_activations]\n        hidden_units = [input_dim] + hidden_units\n        for idx in range(len(hidden_units) - 1):\n            dense_layers.append(nn.Linear(hidden_units[idx], hidden_units[idx + 1], bias=use_bias))\n            if batch_norm:\n                dense_layers.append(nn.BatchNorm1d(hidden_units[idx + 1]))\n            if hidden_activations[idx]:\n                dense_layers.append(hidden_activations[idx])\n            if dropout_rates[idx] > 0:\n                dense_layers.append(nn.Dropout(p=dropout_rates[idx]))\n        if output_dim is not None:\n            dense_layers.append(nn.Linear(hidden_units[-1], output_dim, bias=use_bias))\n        if final_activation is not None:\n            dense_layers.append(set_activation(final_activation))\n        self.dnn = nn.Sequential(*dense_layers)  # * used to unpack list\n\n    def forward(self, inputs):\n        return self.dnn(inputs)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:08.473312Z","iopub.execute_input":"2022-09-17T18:03:08.473848Z","iopub.status.idle":"2022-09-17T18:03:08.488064Z","shell.execute_reply.started":"2022-09-17T18:03:08.473806Z","shell.execute_reply":"2022-09-17T18:03:08.487099Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# 相关函数\ndef set_device(gpu=-1):\n    if gpu >= 0 and torch.cuda.is_available():\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n        device = torch.device(f\"cuda:{gpu}\")\n    else:\n        device = torch.device(\"cpu\")\n    return device\n    \ndef set_activation(activation):\n    if isinstance(activation, str):\n        if activation.lower() == \"relu\":\n            return nn.ReLU()\n        elif activation.lower() == \"sigmoid\":\n            return nn.Sigmoid()\n        elif activation.lower() == \"tanh\":\n            return nn.Tanh()\n        else:\n            return getattr(nn, activation)()\n    else:\n        return activation\n    \ndef get_dnn_input_dim(enc_dict, embedding_dim):\n    num_sparse = 0\n    num_dense = 0\n    for col in enc_dict.keys():\n        if 'min' in enc_dict[col].keys():\n            num_dense+=1\n        elif 'vocab_size' in enc_dict[col].keys():\n            num_sparse+=1\n    return num_sparse * embedding_dim + num_dense\n\ndef get_linear_input(enc_dict, data):\n    res_data = []\n    for col in enc_dict.keys():\n        if 'min' in enc_dict[col].keys():\n            res_data.append(data[col])\n    res_data = torch.stack(res_data,axis=1)\n    return res_data","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:08.492441Z","iopub.execute_input":"2022-09-17T18:03:08.493090Z","iopub.status.idle":"2022-09-17T18:03:08.509523Z","shell.execute_reply.started":"2022-09-17T18:03:08.493037Z","shell.execute_reply":"2022-09-17T18:03:08.507865Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# 定义我们的模型\nclass TPSModel(nn.Module):\n    def __init__(self,\n                embedding_dim=16,\n                hidden_units=[64,32,16],\n                enc_dict=None,\n                hidden_activations='relu',\n                dropout_rates=0,\n                loss_fun='torch.nn.BCELoss()'):\n        super(TPSModel, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_units = hidden_units\n        self.enc_dict = enc_dict\n        self.hidden_activations = hidden_activations\n        self.dropout_rates = dropout_rates\n        self.loss_fun = eval(loss_fun)\n        \n        self.embedding_layer = EmbeddingLayer(enc_dict=self.enc_dict, \n                                              embedding_dim=self.embedding_dim)\n        \n        self.dnn_input_dim = get_dnn_input_dim(enc_dict=self.enc_dict,\n                                              embedding_dim=self.embedding_dim)\n        \n        self.dnn = MLP_Layer(input_dim=self.dnn_input_dim,\n                            output_dim=1,\n                            hidden_units=self.hidden_units,\n                            hidden_activations=self.hidden_activations,\n                            dropout_rates=self.dropout_rates)\n        \n    def forward(self,data):\n        sparse_embedding = self.embedding_layer(data)\n        sparse_embedding = torch.flatten(sparse_embedding, start_dim=1)\n\n        dense_input = get_linear_input(enc_dict=self.enc_dict, data=data)\n        dnn_input = torch.cat([sparse_embedding, dense_input], axis=1)\n\n        y_pred = self.dnn(dnn_input).sigmoid()\n        loss = self.loss_fun(y_pred.squeeze(-1), data['target'])\n        output_dict = {'pred':y_pred, 'loss':loss}\n        return output_dict","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:08.511476Z","iopub.execute_input":"2022-09-17T18:03:08.512018Z","iopub.status.idle":"2022-09-17T18:03:08.530626Z","shell.execute_reply.started":"2022-09-17T18:03:08.511968Z","shell.execute_reply":"2022-09-17T18:03:08.529256Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# 训练、验证、测试的pipeline\ndef train_model(model, train_loader, optimizer, device, metric_list=['roc_auc_score','log_loss']):\n    model.train()\n    pred_list = []\n    label_list = []\n    pbar = tqdm(train_loader)\n    for data in pbar:\n\n        for key in data.keys():\n            data[key] = data[key].to(device)\n\n        output = model(data)\n        pred = output['pred']\n        loss = output['loss']\n\n        loss.backward()\n        optimizer.step()\n        model.zero_grad()\n\n        pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n        label_list.extend(data['target'].squeeze(-1).cpu().detach().numpy())\n        pbar.set_description(\"Loss {}\".format(loss))\n\n    res_dict = dict()\n    for metric in metric_list:\n        if metric =='log_loss':\n            res_dict[metric] = log_loss(label_list, pred_list, eps=1e-7)\n        else:\n            res_dict[metric] = eval(metric)(label_list, pred_list)\n\n    return res_dict\n\n\ndef valid_model(model, valid_loader, device, metric_list=['roc_auc_score','log_loss']):\n    model.eval()\n    pred_list = []\n    label_list = []\n\n    for data in (valid_loader):\n\n        for key in data.keys():\n            data[key] = data[key].to(device)\n\n        output = model(data)\n        pred = output['pred']\n\n        pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n        label_list.extend(data['target'].squeeze(-1).cpu().detach().numpy())\n\n    res_dict = dict()\n    for metric in metric_list:\n        if metric =='log_loss':\n            res_dict[metric] = log_loss(label_list, pred_list, eps=1e-7)\n        else:\n            res_dict[metric] = eval(metric)(label_list, pred_list)\n\n    return res_dict\n\n\ndef test_model(model, test_loader, device):\n    model.eval()\n    pred_list = []\n\n    for data in tqdm(test_loader):\n\n        for key in data.keys():\n            data[key] = data[key].to(device)\n\n        output = model(data)\n        pred = output['pred']\n        pred_list.extend(pred.squeeze().cpu().detach().numpy())\n\n    return np.array(pred_list)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:08.532501Z","iopub.execute_input":"2022-09-17T18:03:08.533240Z","iopub.status.idle":"2022-09-17T18:03:08.553988Z","shell.execute_reply.started":"2022-09-17T18:03:08.533202Z","shell.execute_reply":"2022-09-17T18:03:08.552127Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# dataloader\ntrain_loader = D.DataLoader(train_dataset, batch_size=config['batch'], shuffle=True, num_workers=0)\nvalid_loader = D.DataLoader(valid_dataset, batch_size=config['batch'], shuffle=False, num_workers=0)\ntest_loader = D.DataLoader(test_dataset, batch_size=config['batch'], shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:08.556158Z","iopub.execute_input":"2022-09-17T18:03:08.559282Z","iopub.status.idle":"2022-09-17T18:03:08.572685Z","shell.execute_reply.started":"2022-09-17T18:03:08.559233Z","shell.execute_reply":"2022-09-17T18:03:08.570913Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"model = TPSModel(enc_dict=enc_dict)\ndevice = set_device(config['device'])\noptimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:15.228950Z","iopub.execute_input":"2022-09-17T18:03:15.229392Z","iopub.status.idle":"2022-09-17T18:03:15.280304Z","shell.execute_reply.started":"2022-09-17T18:03:15.229355Z","shell.execute_reply":"2022-09-17T18:03:15.278613Z"},"trusted":true},"execution_count":74,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/1335583397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTPSModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/780294550.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding_dim, hidden_units, enc_dict, hidden_activations, dropout_rates, loss_fun)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         self.embedding_layer = EmbeddingLayer(enc_dict=self.enc_dict, \n\u001b[0;32m---> 19\u001b[0;31m                                               embedding_dim=self.embedding_dim)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         self.dnn_input_dim = get_dnn_input_dim(enc_dict=self.enc_dict,\n","\u001b[0;32m/tmp/ipykernel_17/3196746991.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, enc_dict, embedding_dim)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 self.embedding_layer.update({col : nn.Embedding(\n\u001b[1;32m     17\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 )})\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"],"ename":"TypeError","evalue":"empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n","output_type":"error"}]},{"cell_type":"code","source":"# 训练\nfor i in range(config['epoch']):\n    #模型训练\n    train_metirc = train_model(model,train_loader,optimizer=optimizer,device=device)\n    #模型验证\n    valid_metric = valid_model(model,valid_loader,device)\n\n    print(\"Train Metric:\")\n    print(train_metirc)\n    print(\"Valid Metric:\")\n    print(valid_metric)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:03:18.026300Z","iopub.execute_input":"2022-09-17T18:03:18.026735Z","iopub.status.idle":"2022-09-17T18:03:18.052165Z","shell.execute_reply.started":"2022-09-17T18:03:18.026698Z","shell.execute_reply":"2022-09-17T18:03:18.050246Z"},"trusted":true},"execution_count":75,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3396901432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#模型训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_metirc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#模型验证\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalid_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# 测试\ny_pre = test_model(model, test_loader, device)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:04:07.709109Z","iopub.execute_input":"2022-09-17T18:04:07.709562Z","iopub.status.idle":"2022-09-17T18:04:07.733197Z","shell.execute_reply.started":"2022-09-17T18:04:07.709476Z","shell.execute_reply":"2022-09-17T18:04:07.731079Z"},"trusted":true},"execution_count":76,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/128261299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 测试\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# 写入结果\n","metadata":{},"execution_count":null,"outputs":[]}]}